import os
import warnings
import pandas as pd
from sqlalchemy import create_engine
from dotenv import load_dotenv
import json
import google.generativeai as genai

# Configuration
warnings.filterwarnings('ignore')
load_dotenv()

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
GEMINI_MODEL_NAME = 'gemini-1.5-flash'
genai.configure(api_key=GEMINI_API_KEY)
MODEL = genai.GenerativeModel(GEMINI_MODEL_NAME)

EDA_DIR = os.getenv("EDA_DIR")
eda_path = os.path.join(EDA_DIR, "outputs", "global_trade")

# DB connection
PG_USER = os.getenv("POSTGRES_USER")
PG_PASSWORD = os.getenv("POSTGRES_PASSWORD")
PG_DB = os.getenv("POSTGRES_DB")
PG_HOST = os.getenv("POSTGRES_HOST")
PG_PORT = os.getenv("POSTGRES_PORT")

engine = create_engine(f"postgresql+psycopg2://{PG_USER}:{PG_PASSWORD}@{PG_HOST}:{PG_PORT}/{PG_DB}")

# Top 5 Year-over-Year (YoY) Decreased Export Items
def load_top5_decreased_export_items_data(engine):
    query = """
    SELECT * 
    FROM trade_global_export_decrease_items_top5_processed
    ORDER BY full_label, indicator, value
    """
    return pd.read_sql(query, engine)

# Top 5 Year-over-Year (YoY) Increased ExportItems
def load_top5_increased_export_items_data(engine):
    query = """
    SELECT * 
    FROM trade_global_export_increase_items_top5_processed
    ORDER BY full_label, indicator, value
    """
    return pd.read_sql(query, engine)

# Top 5 Year-over-Year Trade Increased Countries
def load_top5_increased_export_countries_data(engine):
    query = """
    SELECT * 
    FROM trade_global_trade_variation_top5_processed
    ORDER BY country,indicator, value
    """
    return pd.read_sql(query, engine)

# Top 5 Trading Partners
def load_top5_trading_partners_data(engine):
    query = """
    SELECT * 
    FROM trade_global_trade_processed
    ORDER BY rank, country, indicator, value
    """
    return pd.read_sql(query, engine)

# Shipping Index
def load_shipping_index_data(engine):
    query = """
    SELECT date, indicator, value, unit 
    FROM trade_shipping_indices_processed
    WHERE value IS NOT NULL
    ORDER BY date, indicator 
    """
    return pd.read_sql(query, engine)

# English translation dictionary
eng_commodity_name = {
    'ì²œì—°ê°€ìŠ¤': "Natural Gas",
    'ìœ ì—°íƒ„' : 'Bituminous Coal',
    'ì„ìœ ì™€ ì—­ì²­ìœ (ç€é‘æ²¹)(ì›ìœ ë¡œ í•œì •í•œë‹¤)': 'Crude Oil',
    'ê²½ì§ˆìœ (è¼•è³ªæ²¹)ì™€ ì¡°ì œí’ˆ': 'Light Oil and Preparations',
    'ì „ê¸°ì—ë„ˆì§€': 'Electrical Energy',
    'ê·¸ ë°–ì˜ ì„íƒ„': 'Other Coal',
    'ë©”ëª¨ë¦¬': 'Memory Modules (Electronic Components)',
    'ì‘ê²°ì‹œí‚¤ì§€ ì•Šì€ ê²ƒ': 'Unagglomerated Iron Ores and Concentrates',
    'ê¸°ì–µì¥ì¹˜': 'Data Storage Devices',
    'íœ´ëŒ€ìš© ìë™ìë£Œì²˜ë¦¬ê¸°ê³„': 'Portable Data Processing Machines (<10kg)',
    'ìŒê·¹ê³¼ ìŒê·¹ì˜ í˜•ì¬': 'Refined Copper Cathodes and Sections',
    'ì²˜ë¦¬ì¥ì¹˜(ì†Œí˜¸ ì œ8471.41í˜¸ë‚˜ ì œ8471.49í˜¸ ì™¸ì˜ ê²ƒìœ¼ë¡œì„œ ê¸°ì–µì¥ì¹˜ã†ì…ë ¥ì¥ì¹˜ã†ì¶œë ¥ì¥ì¹˜ ì¤‘ í•œ ê°€ì§€ë‚˜ ë‘ ê°€ì§€ ì¥ì¹˜ë¥¼ ë™ì¼ í•˜ìš°ì§• ì†ì— ë‚´ì¥í•œ ê²ƒì¸ì§€ì— ìƒê´€ì—†ë‹¤)': 'Processing Units (non-8471.41/49)',
    'ë©´ì—­ë¬¼í’ˆ(ì¼ì •í•œ íˆ¬ì—¬ëŸ‰ìœ¼ë¡œ í•œ ê²ƒ, ì†Œë§¤ìš© ëª¨ì–‘ì´ë‚˜ í¬ì¥ì„ í•œ ê²ƒì— í•œì •í•œë‹¤)': 'Immunological Products (Dosage/Retail)',
    'ê·¸ ë°–ì˜ ì°¨ëŸ‰(ë¶ˆê½ƒì í™”ì‹ í”¼ìŠ¤í†¤ ë‚´ì—°ê¸°ê´€ê³¼ ì¶”ì§„ìš© ëª¨í„°ë¡œì„œì˜ ì „ë™ê¸°ë¥¼ ë‘˜ ë‹¤ ê°–ì¶˜ ê²ƒìœ¼ë¡œì„œ, ì™¸ë¶€ ì „ì›ì— í”ŒëŸ¬ê·¸ë¥¼ ê½‚ì•„ ì¶©ì „í•  ìˆ˜ ìˆëŠ” ë°©ì‹ì˜ ê²ƒì€ ì œì™¸í•œë‹¤)': 'Other Hybrid Vehicles (non-Plug-in)',
    'ë©´ì—­í˜ˆì²­ê³¼ ê·¸ ë°–ì˜ í˜ˆì•¡ ë¶„íšë¬¼': 'Immunological Serum & Blood Fractions',
    'ì œ8471í˜¸ì— í•´ë‹¹í•˜ëŠ” ê¸°ê³„ì˜ ë¶€ë¶„í’ˆê³¼ ë¶€ì†í’ˆ': 'Parts for HS8471 Machines',
    'ê·¸ ë°–ì˜ ì°¨ëŸ‰(ì¶”ì§„ìš© ì „ë™ê¸°ë§Œì„ ê°–ì¶˜ ê²ƒ)': 'Electric Vehicles (Motor Only)',
    'ê´‘ì „ì§€(ëª¨ë“ˆì— ì¡°ë¦½ë˜ì—ˆê±°ë‚˜ íŒ¨ë„ë¡œ êµ¬ì„±ëœ ê²ƒìœ¼ë¡œ í•œì •í•œë‹¤)': 'Photovoltaic Cells (Modules/Panels)',
    'ë¦¬íŠ¬ì´ì˜¨ ì¶•ì „ì§€': 'Lithium-Ion Batteries',
    'í„°ë³´ì œíŠ¸ë‚˜ í„°ë³´í”„ë¡œí ëŸ¬ì˜ ê²ƒ': 'Parts for Turbojets/Turboprops',
    'ë¹„í–‰ê¸°ã†í—¬ë¦¬ì½¥í„°ã†ë¬´ì¸ê¸°ì˜ ê·¸ ë°–ì˜ ë¶€ë¶„í’ˆ': 'Aircraft & UAV Parts'
}

# Top 5 Year-over-Year (YoY) Decreased Export Items
def process_top5_export_decrease_items(df):

    # Map English names with fallback
    df['eng_commodity_name'] = df['commodity_name'].map(eng_commodity_name).fillna(df['commodity_name'])

    # Extract HS code - get the full 6-digit code in parentheses
    df['hs_code'] = df['full_label'].str.extract(r'\((\d{6})\)')[0]

    # Combine for full label using English names with parentheses around HS code
    df['commodity_full_name'] = df['eng_commodity_name'] + '(' + df['hs_code'] + ')'

    # Pivot to combine amount and YoY into one row
    export_decreased_items_pivoted = df.pivot_table(
        index=["date", "country", "commodity_full_name", "change_type"],
        columns="indicator",
        values="value",
        aggfunc="first"
    ).reset_index()

    # Ensure we get the top 5 unique items by YoY change
    export_decreased_items_pivoted = export_decreased_items_pivoted.sort_values('export_yoy').head(5)

    # Rename columns
    export_decreased_items_pivoted = export_decreased_items_pivoted.rename(columns={
        "export_amount": "export_value_thousand_usd",
        "export_yoy": "yoy_change_percent"
    })

    return export_decreased_items_pivoted

# Top 5 Year-over-Year (YoY) Increased Export Items
def process_top5_export_increase_items(df):

    # Map English names with fallback
    df['eng_commodity_name'] = df['commodity_name'].map(eng_commodity_name).fillna(df['commodity_name'])

    # Extract HS code - get the full 6-digit code in parentheses
    df['hs_code'] = df['full_label'].str.extract(r'\((\d{6})\)')[0]

    # Combine for full label using English names with parentheses around HS code
    df['commodity_full_name'] = df['eng_commodity_name'] + '(' + df['hs_code'] + ')'

    # Pivot to combine amount and YoY into one row
    export_increased_items_pivoted = df.pivot_table(
        index=["date", "country", "commodity_full_name", "change_type"],
        columns="indicator",
        values="value",
        aggfunc="first"
    ).reset_index()

    # Ensure we get the top 5 unique items by YoY change
    export_increased_items_pivoted = export_increased_items_pivoted.sort_values('export_yoy', ascending=False).head(5)

    # Rename columns
    export_increased_items_pivoted = export_increased_items_pivoted.rename(columns={
        "export_amount": "export_value_thousand_usd",
        "export_yoy": "yoy_change_percent"
    })

    return export_increased_items_pivoted

# Top 5 Year-over-Year Trade Increasesd Countries
def process_top5_export_increase_countries(df):

    export_increased_countries_pivoted  = df.pivot_table(
        index=["date", "country", "partner"],
        columns="indicator",
        values="value",
        aggfunc="first"
    ).reset_index()

    export_increased_countries_pivoted = export_increased_countries_pivoted.rename(columns={
        "export_amount": "export_value_thousand_usd",
        "export_yoy": "yoy_change_percent",
        "export_share": "export_share_percent",
        "import_share": "import_share_percent"
    })

    return export_increased_countries_pivoted

# Top 5 Trading Partners
def process_top5_trade_partners(df):
    top5_trade_partners_pivoted = df.pivot_table(
        index=["date", "country", "partner", "rank"],
        columns="indicator",
        values="value",
        aggfunc="first"
    ).reset_index()

    top5_trade_partners_pivoted = top5_trade_partners_pivoted.rename(columns={
        "export_amount": "export_value_thousand_usd",
        "export_yoy": "yoy_change_percent",
        "export_share": "export_share_percent",
        "import_share": "import_share_percent"
    })

    return top5_trade_partners_pivoted

# Shipping Index
def process_shipping_index(df):
    df = df.copy()
    df['date'] = pd.to_datetime(df['date'])
    df['value'] = pd.to_numeric(df['value'], errors='coerce')
    
    # Drop rows with missing values
    df = df.dropna(subset=["value", "indicator"])
    
    # Debug info
    print(f"Shipping index data shape: {df.shape}")
    print(f"Unique indicators: {df['indicator'].unique()}")
    print(f"Date range: {df['date'].min()} to {df['date'].max()}")

    # Pivot
    shipping_index_pivoted = df.pivot_table(
        index="date",
        columns="indicator",
        values="value",
        aggfunc="mean"
    ).sort_index()
    
    print(f"Pivoted shipping index shape: {shipping_index_pivoted.shape}")
    print(f"Pivoted columns: {shipping_index_pivoted.columns.tolist()}")

    return shipping_index_pivoted

# Correlation Analysis
def correlation_analysis(shipping_index_pivoted):
    combined_filled = shipping_index_pivoted.fillna(method='ffill').fillna(method='bfill')
    correlation_matrix = combined_filled.corr()

    return correlation_matrix

# Volatility Analysis
def three_month_volatility_analysis(df):
    df = df.copy()
    df['date'] = pd.to_datetime(df['date'])
    df['value'] = pd.to_numeric(df['value'], errors='coerce')

    # Fill missing values
    df_filled = df.fillna(method='ffill').fillna(method='bfill')

    # Set 'date' as the index and sort it
    df_filled = df_filled.set_index('date').sort_index()
    
    # Debug info
    print(f"Volatility analysis - data shape: {df_filled.shape}")
    print(f"Volatility analysis - date range: {df_filled.index.min()} to {df_filled.index.max()}")

    # Resample the data to a monthly frequency and calculate the mean for each month
    monthly_mean = df_filled.resample('M')['value'].mean().to_frame()
    
    print(f"Monthly mean shape: {monthly_mean.shape}")

    # Apply a rolling 3-month standard deviation (volatility) calculation
    rolling_3m_volatility = monthly_mean.rolling(window=3).std()
    
    print(f"Rolling volatility shape: {rolling_3m_volatility.shape}")
    print(f"Non-null volatility values: {rolling_3m_volatility['value'].notna().sum()}")

    return rolling_3m_volatility

# Save
def save_trade_eda_outputs(
    df_decrease_items,
    df_increase_items,
    df_increase_countries,
    df_top5_partners,
    df_shipping_index,
    output_dir
):
    os.makedirs(output_dir, exist_ok=True)

    # Export Decrease Items
    processed_decrease = process_top5_export_decrease_items(df_decrease_items)
    processed_decrease.to_csv(os.path.join(output_dir, "export_decrease_items_top5.csv"), index=False)

    # Export Increase Items
    processed_increase = process_top5_export_increase_items(df_increase_items)
    processed_increase.to_csv(os.path.join(output_dir, "export_increase_items_top5.csv"), index=False)

    # Export Increase Countries
    processed_increase_countries = process_top5_export_increase_countries(df_increase_countries)
    processed_increase_countries.to_csv(os.path.join(output_dir, "export_increase_countries_top5.csv"), index=False)

    # Top 5 Trade Partners
    processed_partners = process_top5_trade_partners(df_top5_partners)
    processed_partners.to_csv(os.path.join(output_dir, "trade_partners_top5.csv"), index=False)

    # Shipping Index: Pivoted
    try:
        shipping_index_pivoted = process_shipping_index(df_shipping_index)
        if not shipping_index_pivoted.empty:
            shipping_index_pivoted.to_csv(os.path.join(output_dir, "shipping_index_pivoted.csv"))
            print(f"âœ… Shipping index pivoted saved: {shipping_index_pivoted.shape}")
        else:
            print("âš ï¸ Warning: Shipping index pivoted is empty")
    except Exception as e:
        print(f"âŒ Error processing shipping index pivoted: {e}")

    # Correlation Matrix
    try:
        if not shipping_index_pivoted.empty:
            corr_matrix = correlation_analysis(shipping_index_pivoted)
            corr_matrix.to_csv(os.path.join(output_dir, "shipping_index_correlation.csv"))
            print(f"âœ… Correlation matrix saved: {corr_matrix.shape}")
        else:
            print("âš ï¸ Warning: Cannot calculate correlation - shipping index is empty")
    except Exception as e:
        print(f"âŒ Error processing correlation matrix: {e}")

    # Rolling 3-Month Volatility
    try:
        rolling_volatility = three_month_volatility_analysis(df_shipping_index)
        if not rolling_volatility.empty:
            rolling_volatility.to_csv(os.path.join(output_dir, "shipping_index_3m_volatility.csv"))
            print(f"âœ… Rolling volatility saved: {rolling_volatility.shape}")
        else:
            print("âš ï¸ Warning: Rolling volatility is empty")
    except Exception as e:
        print(f"âŒ Error processing rolling volatility: {e}")

    print(f"âœ… Trade EDA outputs saved to: {output_dir}")

    # Key insights for AI analysis
    key_insights = {
        "summary_statistics": {
            "Top 5 YoY Decrease Items": processed_decrease[["commodity_full_name", "export_value_thousand_usd", "yoy_change_percent"]]
                .sort_values("yoy_change_percent")
                .head(5)
                .to_dict(orient="records"),

            "Top 5 YoY Increase Items": processed_increase[["commodity_full_name", "export_value_thousand_usd", "yoy_change_percent"]]
                .sort_values("yoy_change_percent", ascending=False)
                .head(5)
                .to_dict(orient="records"),

            "Top 5 Export Increase Countries": processed_increase_countries[["country", "partner", "export_value_thousand_usd", "yoy_change_percent"]]
                .sort_values("yoy_change_percent", ascending=False)
                .head(5)
                .to_dict(orient="records"),

            "Top Trade Partners by Export Value": processed_partners[["country", "partner", "export_value_thousand_usd"]]
                .sort_values("export_value_thousand_usd", ascending=False)
                .head(5)
                .to_dict(orient="records")
        },

        "shipping_index": {
            "correlation_matrix": corr_matrix.round(2).to_dict(),
            "volatility_3m_std": rolling_volatility.round(2).dropna(how='all').tail(1).to_dict(orient="records")[0]
        }
    }

    # Save JSON
    with open(os.path.join(output_dir, "key_insights.json"), "w", encoding='utf-8') as f:
        json.dump(key_insights, f, indent=2, ensure_ascii=False)

    return key_insights

# Gemini Insight
def generate_insights(key_insights, output_dir):
    try:
        summary_stats = key_insights.get("summary_statistics", {})
        shipping_metrics = key_insights.get("shipping_index", {})

        prompt = f"""
**Role**: You are a senior economic strategist analyzing cross-sector ripple effects. Extract non-obvious implications from the data below.

**Data Inputs**:
1. Summary Statistics:
{json.dumps(summary_stats, indent=2)}

2. Key Metrics:
{json.dumps(shipping_metrics, indent=2)}


### **Required Output Format**
## Global Trade Sector Second-Order Effect Analysis

### Core Trend
â€¢ Global Trade: [TREND SUMMARY IN 5â€“10 WORDS]  
â€¢ **Direct Impact**: [IMMEDIATE OUTCOME IN 1 SENTENCE]

### Hidden Effects
1. **[EFFECT 1 NAME]**
   - *Catalyst*: [PRIMARY DRIVER]
   - *Transmission*: [HOW IT SPREADS THROUGH SYSTEM]
   - *Evidence*: [DATA POINT OR HISTORICAL PRECEDENT]

2. **[EFFECT 2 NAME]**
   - *Catalyst*: [PRIMARY DRIVER]
   - *Transmission*: [HOW IT SPREADS THROUGH SYSTEM]
   - *Evidence*: [DATA POINT OR HISTORICAL PRECEDENT]

### Strategic Recommendations
ğŸ›  **Immediate Actions**: [CONCRETE STEPS]
ğŸ“Š **Monitoring Metrics**: [KEY INDICATORS]
ğŸ¯ **Long-term Strategy**: [STRATEGIC DIRECTION]

### Risk Assessment
âš ï¸ **High Risk**: [CRITICAL CONCERN]
âš ï¸ **Medium Risk**: [MODERATE CONCERN]
âš ï¸ **Low Risk**: [MINOR CONCERN]

### Market Intelligence
ğŸ“ˆ **Bullish Signals**: [POSITIVE INDICATORS]
ğŸ“‰ **Bearish Signals**: [NEGATIVE INDICATORS]
ğŸ”„ **Neutral Factors**: [BALANCED ELEMENTS]

**Analysis Guidelines**:
- Focus on actionable intelligence
- Consider global geopolitical dynamics
- Assess Korea's competitive positioning
- Identify emerging trends and risks
- Provide specific, measurable recommendations
"""
        response = MODEL.generate_content(prompt)
        gemini_insight = response.text.strip()

        with open(f"{output_dir}/gemini_insight_gloal_trade.txt", "w", encoding="utf-8") as f:
            f.write(gemini_insight)

        print("âœ… Gemini insights generated and saved.")

    except Exception as e:
        print(f"âŒ Gemini insight generation failed: {e}")

# Main
def main():
    # Load data from database
    df_decrease_items = load_top5_decreased_export_items_data(engine)
    df_increase_items = load_top5_increased_export_items_data(engine)
    df_increase_countries = load_top5_increased_export_countries_data(engine)
    df_top5_partners = load_top5_trading_partners_data(engine)
    df_shipping_index = load_shipping_index_data(engine)

    # Save all processed outputs and generate key insights
    key_insights = save_trade_eda_outputs(
        df_decrease_items,
        df_increase_items,
        df_increase_countries,
        df_top5_partners,
        df_shipping_index,
        eda_path
    )

    # Generate Gemini insight text based on key stats
    generate_insights(key_insights, eda_path)

    print(f"\nâœ… All data saved to: {eda_path}")
    print("="*50)

if __name__ == "__main__":
    main()
